![Deep Learning: il corso pratico](https://raw.githubusercontent.com/ProfAI/dl00/master/res/banner.jpg)

## [Deep Learning e Reti Neurali con Python: il Corso Pratico](https://www.udemy.com/deep-learning-pratico/?couponCode=PROFAI_GITHUB)

#### L'Intelligenza Artificiale sta facendo progressi esponenziali, avanzando come nessuna tecnologia aveva mai fatto prima nella storia dell'uomo, e il merito è di un solo e unico campo: il Deep Learning. ####

Il Deep Learning è l'insieme di metodi utilizzati per addestrare le Reti Neurali Artificiali, un particolare modello del Machine Learning che hanno rivoluzionato l'intero settore.

Applicazione pratiche di Deep Learning e Reti Neurali Artificiali sono già intorno a noi:

Le self-driving cars che cambieranno come mai prima d'ora la mobilità urbana.

Gli assistenti virtuali come Alexa di Amazon e Google Home che sono sempre più presenti all'interno delle nostre abitazioni.

Sistemi intelligenti come IBM Watson che ogni giorno aiutano medici a fare diagnosi migliori salvando vite umane.

In questo corso esploreremo il funzionamento del Deep Learning e impareremo insieme a creare i nostri modelli di Reti Neurali Artificiali utilizzando Python e Keras su Tensorflow per risolvere problemi differenti, come:

 - Identificare tumori maligni.
 - Riconoscere capi di abbigliamento nelle foto.
 - Classificare recensioni come positive o negative.

### Hai già seguito il nostro primo corso sul Machine Learning con Python o hai già esperienza con il Machine Learning ? ###

Allora sei pronto per addentrarti più in profondità con lo studio del Deep Learning e delle Reti Neurali Artificiali.

### Non hai mai avuto a che fare con il Machine Learning prima d'ora ? ###

Non temere, abbiamo pensato anche a te, all'interno del corso troverai delle sezioni specifiche in cui ti verranno forniti tutti i prerequisiti sul Machine Learning necessari per poterti cimentare con le Reti Neurali Artificiali.

L'unica cosa di cui hai bisogno per affrontare questo corso è un minimo di conoscenza di programmazione in Python e basi di matematica da scuola superiore.

### Cosa faremo durante il corso ? ###

Nella prima sezione osserveremo a cosa serve esattamente il Deep Learning e alcune delle sue applicazioni più importanti. Vedremo insieme quali sono i linguaggi più popolari per il Deep Learning e quali sono le librerie per Python che ci permettono di creare Reti Neurali Artificiali.

Dopo questa breve introduzione inizieremo subito a sporcarci le mani, osservando il funzionamento di una rete neurale artificiale per poi crearne una insieme, al fine di riconoscere tumori maligni partendo da informazioni estratte da degli esami radiologici.

Proseguiremo il corso studiando tutte le principali tecniche utilizzate per addestrare una rete neurale artificiale, come:

Gradient descent, nelle sue varianti Full batch, Mini Batch e Stochastic.

L'utilizzo di Momentum e Nesterov Momentum.

 - AdaGrad.
 - RMSprop.
 - AdaDelta.
 - Adam, Nadam e Adamax

Utilizzeremo le informazioni acquisite per addestrare una rete neurale artificiale in grado di riconoscere calzature, capi di abbigliamento e accessori nelle fotografie, utilizzando il dataset Fashion-MNIST.

Nella sezione seguente introdurremo il problema dell'overfitting nelle reti neurali artificiali e vedremo come contrastarlo con tecniche generiche come regolarizzazione L1 e L2 e specifiche per le reti neurali come il Dropout. Qui sfrutteremo il dataset delle recensioni di film dell'Internet Movie Database (IMDB) per creare una rete neurale in grado di comprendere in maniera autonoma se una recensione è positiva o negativa.

Il processo di addestramento di una Rete Neurale Artificiale può essere molto dispendioso, in termini di tempo e risorse di calcolo, per questo abbiamo creato una sezione apposita per mostrarti come velocizzare il processo sfruttando la parallelizzazione delle GPUs e servizi in Cloud come Google Colaboratory e Amazon AWS.

In seguito vedremo un'altra tipologia di reti neurali utilissima  se il nostri dati sono una sequenza, come testi, audio, video e serie storiche, cioè le Reti Neurali Ricorrenti nelle varianti Vanilla Recurrent Neural Network, Long short-term memory (LSTM) e Gated Recurrent Unit (GRU). Inoltre vedremo come creare rappresentazioni vettoriali di testi utilizzando il Word Embedding.

Nell'ultima sezione pratica combineremo strati convoluzionali e ricorrenti per creare una Convolutional Long Short-Term Memory Network.

Al termine del corso ti verranno forniti alcuni consigli su come proseguire la tua avventura nel campo del Deep Learning, sia sotto un punto di vista pratico che teorico.

**Artificial Intelligence, Machine Learning e Deep Learning sono in continua evoluzione, quindi dopo l'ultima sezione non considerare questo corso concluso, perché verrà aggiornato costantemente con nuovi contenuti. Ad ogni nuovo aggiornamento riceverai una notifica tramite email.**

I contenuti in programma per il prossimo aggiornamento sono: 

 - Eseguire predizioni su serie storiche utilizzando le reti neurali ricorrenti.
 - Tecniche di dataset augmentation e generatori di immagini

### [IL CORSO COMPLETO E' DISPONIBILE SU UDEMY A SOLI 9.99 € CLICCANDO SU QUESTO LINK](https://www.udemy.com/deep-learning-pratico/?couponCode=PROFAI_GITHUB)

### Letture aggiuntive

- [Learning representation by back-propagating errors](https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf)
- [Understanding the difficulty of training deep feedforward neural networks](https://www.researchgate.net/publication/215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks)
- [On the importance of initialization and momentum in deep learning](http://proceedings.mlr.press/v28/sutskever13.pdf)
- [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
- [ADADELTA: AN ADAPTIVE LEARNING RATE METHOD](https://arxiv.org/pdf/1212.5701)
- [ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](https://arxiv.org/pdf/1412.6980)
- [Incorporating Nesterov Momentum into Adam](https://cs229.stanford.edu/proj2015/054_report.pdf)
- [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747)
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
- [Gradient-Based Learning Applied to Document Recognition](https://axon.cs.byu.edu/~martinez/classes/678/Papers/Convolution_nets.pdf)
- [Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition](https://www.researchgate.net/publication/221080312_Evaluation_of_Pooling_Operations_in_Convolutional_Architectures_for_Object_Recognition)
- [Long Short-Term Memory](https://www.researchgate.net/publication/13853244_Long_Short-Term_Memory)
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)
- [On the difficulty of training Recurrent Neural Networks](https://arxiv.org/pdf/1211.5063)
- [Understanding LSTM Networks](https://web.stanford.edu/class/cs379c/archive/2018/class_messages_listing/content/Artificial_Neural_Network_Technology_Tutorials/OlahLSTM-NEURAL-NETWORK-TUTORIAL-15.pdf)
- [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555)
- [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/pdf/1409.1259)
- [Long-term Recurrent Convolutional Networks for Visual Recognition and Description](https://arxiv.org/pdf/1411.4389)
- [ImageNet Classification with Deep Convolutional Neural Networks](https://www.cs.cmu.edu/~epxing/Class/10715-14f/reading/imagenet.pdf)